# Logistic Regression
---
- When using a [[42.02 Linear Regression|Linear Regression]] model, sometimes the output produced may not be in the range of zero to one which can be quite the problem since in order to make predictions we require probabilities and probabilities need to be in the range of zero to one
- In order to overcome this issue, we may use logistic regression so that the output is always in the range of zero to one (aka **calibrated**)
- This technique is also useful for classification tasks
- The general idea is to simply pass the outputs of the linear regression model through a squashing function such as sigmoid or tanh
- A key difference between linear and logistic regression is that logistic regression uses `log loss` instead of MSE $$LogLoss = \sum_{(x,y)\in D} -y\,log(y') - (1 - y)\,log(1 - y')$$
> [!tip] Shannon's Entropy
> - This [article](https://machinelearningmastery.com/what-is-information-entropy/) explains the crux of *information theory* and delves into Shannon's Entropy
> - The loss function used in logistic regression is quite similar

- The function used is asymptotic and thus necessitates regularisation since the squashing performed can cause the values to be skewed

> [!important] Benefits of Linear Logistic Regression
> 1. Extremely efficient
> 2. Very fast training and prediction times