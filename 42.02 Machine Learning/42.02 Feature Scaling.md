# Feature Scaling
---
- **Feature scaling** is a technique used to standardise the independent features present in the data to a fixed range
- When the dataset has features whose magnitudes are significantly different from one another, the features whose magnitudes are significantly greater tend to have a greater influence on the predictions made
- In order to combat this, feature scaling can be applied to bring down the numerical largeness of features and make sure all the different features have values within a similar range
- In the below example, age is in range of 10s while the salary is in the range of 10000s which cause the salary feature to have a profound influence on the predictions ![[Screenshot 2023-07-06 at 2.38.40 PM.png]]
- Irregular scale across features can hamper the functioning of some of the widely used ML algorithms and hence it is advisable to scale the features appropriately prior to passing the data through an algorithm
- Types:
	1. [[42.02 Standardisation|Standardisation]]
	2. [[42.02 Normalisation|Normalisation]]
- Standardisation/ Normalisation does not influence the information conveyed by/ shape of the data itself and only modifies its scale
- Things to watch out for:
	- is feature scaling required for the problem
	- if scaling is required, in most cases standardisation will be useful
	- min-max normalisation is done only when the minimum and maximum of the feature values is known to us beforehand (example: in image processing — CNN — each pixel has an RGB value in the range $[0,255]$)

