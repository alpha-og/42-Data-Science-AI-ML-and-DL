# Simple Linear Regression
---
- **Simple Linear Regression** problems have a single input and a single output
- Suitable for problems whose inputs and outputs show a somewhat linear relationship
- There are two approaches to solving Simple Linear Regression problems:
	1. Closed Form Solution
	2. Non - Closed Form Solution
## Geometric Intuition
- Simple linear regression Involves finding the equation of a straight line or hyperplane that best fits a set of points. 
- The phrase "best fit" refers to the distance between each point and the line being minimum
- Fundamentally, the model attempts to find the slope ($m$) and y-intercept ($b$) of the line that can be drawn to best represent the training data
- The equation of the line so produced can be used to make predictions by passing inputs ($x$) to obtain the output ($y$)
- The slope is indicative of the weightage/ impact that the input has on the output
- The y-intercept is an offset/ bias introduced into the equation to handle scenarios where the product of weightage and input lead to unreliable/ irrational outputs. This bias makes it so that even if the input term becomes zero due to some reason, the output will not be zero (the utility of the bias may vary in different circumstances)
## Closed Form Solution
- The *closed form solution* to the simple linear regression problem is a direct formula based approach wherein we determine the values of $b$ and $m$ using two formulae
- This method is suitable for datasets whose dimensionality is very less. However, in case of high dimensionality of data, this method can become slow/ inefficient due to the calculations required
- In the following sections we will derive the formula for the closed form solution

**Loss Function**

Let us start by producing a mathematical equation for the **total error** corresponding to the best fit line. Let $d_i$ be the distance of $i^{th}$ datapoint from the best fit line and $N$ be the total number of datapoints. Now, the error can be expressed in the following ways: 
1. $$E=\overset{N}{\underset{i=1}\Sigma}{d_i}$$
2. $$E=\overset{N}{\underset{i=1}\Sigma}{|d_i|}$$
3. $$E=\overset{N}{\underset{i=1}\Sigma}{(d_i)^2}$$

The first expression is not preferred since negative values will cancel out the positive values and reduce the overall error without the model improving any of its hyperparameters. The second expression is not suitable since, although it solves the problem of negative values, it introduces a secondary problem as the modulus function is non-differentiable at the origin and this approach does not facilitate penalisation of outliers. The third expression is widely used and preferred as it converts the negative values to positive while at the same time maintaining differentiability of the error function

The error function so obtained is called the **loss function**. We may further decompose the loss function's distance term and express it as follows: $$E=\overset{N}{\underset{i=1}\Sigma}{(y_i-{\hat{y}_i})^2}$$where $y_i$ is the target variable for the $i^{th}$ datapoint according to the dataset and $\hat{y}_i$ is the predicted value for the target variable for the $i^{th}$ datapoint according to the best fit line. 
We further breakdown the predicted value term into $\hat{y}_i=mx_{i}+ b$ to obtain the following equation: $$E(m,b)=\overset{N}{\underset{i=1}\Sigma}{(y_i-(mx_i+b))^2}$$
#### Graphs
Let us consider two cases:
1. **Case 1:** $b=0$ (the error function is made to be independent of $b$)
2. **Case 2:** $m=1$ or $m=constant$ (the error function is made to be independent of $m$)

According to case 1, the equation for the loss function will be $E(m)=\overset{N}{\underset{i=1}\Sigma}{(y_i-mx_i)^2}$. Mathematically, this equation is a quadratic in terms of $m$ and on plotting the graph of the equation, we see a parabolic curve

According to case 2, the equation for the loss function will be $E(b)=\overset{N}{\underset{i=1}\Sigma}{(y_i-x_i-b)^2}$. Mathematically, this equation is a quadratic in terms of $b$ and on plotting the graph of the equation, we see a parabolic curve

Now, we can also observe that the 3d plot of the loss function as a function of $m$ and $b$ is a 3D parabola. 

> [!warning]
> The graphical plots may not necessarily be parabolic. However, regardless of the shape of the graphs, regression problems always attempt to find the minima of the loss function, i.e, minimise the loss

#### Finding the Minima
Thus in order to reduce or decrease the loss/ error we must minimise the loss function by taking its derivate and equating it to zero. However, since the loss function is dependent on both $m$ and $b$, we must take the partial derivative of the loss function with respect to $m$ and $b$, respectively and equate the partial derivatives to zero to obtain two equations which can be solved to obtain the point of minima

**Obtaining an equation for $b$**

The partial derivative of the loss function with respect to $b$ (considering $m$ to be a constant) is as follows:
$$\begin{align}\frac{\partial{E}}{\partial{b}}&=\frac{\partial{\overset{N}{\underset{i=1}{\Sigma}}(y_{i}-(mx_{i}+b))^{2}}}{\partial{b}}\\
&=\overset{N}{\underset{i=1}{\Sigma}}\frac{\partial{(y_{i}-mx_{i}-b)^2}}{\partial{b}}\\
& =-\overset{N}{\underset{i=1}{\Sigma}}{2(y_{i}-mx_{i}-b)}\\
& =-2\overset{N}{\underset{i=1}{\Sigma}}{(y_{i}-mx_{i}-b)}\end{align}$$

Equating $\frac{\partial{E}}{\partial{b}}$ to zero,

$$\begin{align}
\frac{\partial{E}}{\partial{b}}=0&=-2\overset{N}{\underset{i=1}{\Sigma}}{(y_{i}-mx_{i}-b)}\\
& =\overset{N}{\underset{i=1}{\Sigma}}{(y_{i}-mx_{i}-b)})\\
& = \overset{N}{\underset{i=1}{\Sigma}}(y_{i})-\overset{N}{\underset{i=1}{\Sigma}}(mx_{i})-\overset{N}{\underset{i=1}{\Sigma}}(b)\end{align}$$

Dividing by $N$ on both sides,

$$\begin{align}0&=\frac{\overset{N}{\underset{i=1}{\Sigma}}(y_{i})-\overset{N}{\underset{i=1}{\Sigma}}(mx_{i})-\overset{N}{\underset{i=1}{\Sigma}}(b)}{N}\\
&=\frac{\overset{N}{\underset{i=1}{\Sigma}}(y_{i})}{N}-\frac{\overset{N}{\underset{i=1}{\Sigma}}(mx_{i})}{N}-\frac{\overset{N}{\underset{i=1}{\Sigma}}(b)}{N}\\
&=\frac{\overset{N}{\underset{i=1}{\Sigma}}(y_{i})}{N}-m\frac{\overset{N}{\underset{i=1}{\Sigma}}(x_{i})}{N}-\frac{\overset{N}{\underset{i=1}{\Sigma}}(b)}{N}\\
&=\overline{y}-m\overline{x}-b\end{align}$$

Thus, we obtain $$b=\overline{y}-m\overline{x}$$
**Obtaining an equation for $m$:**

Substituting $b=\overline{y}-m\overline{x}$ in the loss function we obtain $$E(m,b)=\overset{N}{\underset{i=1}\Sigma}{(y_i-mx_i-\overline{y}+m\overline{x})^2}$$
The partial derivative of the loss function with respect to $m$ is as follows:
$$\begin{align}
\frac{\partial{E}}{\partial{m}}&=\frac{\partial{\overset{N}{\underset{i=1}{\Sigma}}(y_i-mx_i-\overline{y}+m\overline{x})^{2}}}{\partial{m}}\\
&=\overset{N}{\underset{i=1}{\Sigma}}\frac{\partial(y_i-mx_i-\overline{y}+m\overline{x})^{2}}{\partial{m}}\\
&=\overset{N}{\underset{i=1}{\Sigma}}{2(y_i-mx_i-\overline{y}+m\overline{x})(\overline{x}-x_i)}\\
&=-2\overset{N}{\underset{i=1}{\Sigma}}{(y_i-mx_i-\overline{y}+m\overline{x})(x_i-\overline{x})}\end{align}$$

Equating $\frac{\partial{E}}{\partial{m}}$ to zero,
$$\begin{align}
\frac{\partial{E}}{\partial{m}}=0 & = -2\overset{N}{\underset{i=1}{\Sigma}}{(y_i-mx_i-\overline{y}+m\overline{x})(x_i-\overline{x})} \\ 
& = \overset{N}{\underset{i=1}{\Sigma}}{(y_i-mx_i-\overline{y}+m\overline{x})(x_i-\overline{x})} \\
& = \overset{N}{\underset{i=1}{\Sigma}}{((y_i-\overline{y})-m(x_i-\overline{x}))(x_i-\overline{x})} \\
& = \overset{N}{\underset{i=1}{\Sigma}}{((y_i-\overline{y})(x_i-\overline{x})-m(x_i-\overline{x})^{2})} \\
& = \overset{N}{\underset{i=1}{\Sigma}}{(y_i-\overline{y})(x_i-\overline{x})}-m\overset{N}{\underset{i=1}{\Sigma}}{(x_i-\overline{x})^{2}}\\
& \Rightarrow m\overset{N}{\underset{i=1}{\Sigma}}{(x_i-\overline{x})^{2}}=\overset{N}{\underset{i=1}{\Sigma}}{(y_i-\overline{y})(x_i-\overline{x})}\end{align}$$

Thus, we obtain $$m=\frac{\overset{N}{\underset{i=1}{\Sigma}}{(y_i-\overline{y})(x_i-\overline{x})}}{\overset{N}{\underset{i=1}{\Sigma}}{(x_i-\overline{x})^{2}}}$$