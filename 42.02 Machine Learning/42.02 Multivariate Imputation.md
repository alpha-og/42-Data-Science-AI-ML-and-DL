# Multivariate Imputation
---
- Multivariate Imputation involves filling of missing values using data from multiple different columns
## KNN Imputation
- Reference video: https://youtu.be/-fK-xEev2I8
- KNN Imputer is used in tandem with the KNN algorithm
- The reasoning behind this technique is that the neighbouring features have an influence on the values of a chosen feature
- KNN Imputer replaces missing values in a column with values from other columns which are most similar to the column in which missing values are present
- The similarity between columns is determined based on euclidean distances
	- Each record can be considered to be corresponding to a point in an $n$ dimensional space where $n$ is the number of features per record
	- Euclidean distances between the record having missing values and every other record are calculated. (the column to be filled is ignored in both records being compared and distance is calculated based on the remaining columns)
	- A record whose euclidean distance from another record is minimum is said to be most similar to each other. 
	- Thus a record (say record "A") whose euclidean distance from the record with missing values (say record "B" with missing value in column 1) is minimum will be the most similar to that record "B" and the data in column 1 from the record "A" is used to impute the missing value in column 1 of record "B"
- In addition, we may also specify the number of records to consider for imputation. If more than one record is specified then the missing value is imputed with their mean
- A scenario may arise where both the records between which euclidean distance is being calculated has missing values. In such cases a special technique is used called [nan_euclidean_distance](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.nan_euclidean_distances.html) which ignores the corresponding features if one or both the records have values missing for a given feature and multiplies the component within the square root with a weight (the weight is the ratio of total features to total available non null features)
- Suitable for small dataset
- **Advantage:** High accuracy
- **Disadvantages:**
	1. Highly calculative and thus requires more computation power
	2. In online learning, the training dataset has to be stored in memory since euclidean distance based comparisons require the original dataset
	3. Slow
## Iterative Imputation
- Reference video: https://youtu.be/a38ehxv3kyk
- aka: Multivariate Imputation by Chained Equations (MICE)
- The intuition behind MICE is that we can use ML models to predict missing values and then use the completed dataset to solve problems using ML
- **Assumption:** Data is MAR (MICE can be used in cases of MCAR or MNAR also, but the results may vary)
- **Advantage:** High accuracy
- **Disadvantages:** 
	1. Slow
	2. Memory hungry
- The MICE algorithm is applied on the inputs only and not on the target column
- **Steps**
	1. Step 1: Apply univariate imputation (preferably use a single method on whichever columns imputation is required) wherever necessary
	2. Step 2: Iterate over each column from left to right performing steps 3-4 (perform the operations in place, i.e the data after modification in steps 3 & 4 must be passed into the loop each time)
	3. Step 3: Replace the values which were previously missing in the current column with NaN (the current column is the selected column of the current iteration)
	4. Step 4: Use any algorithm and predict the NaN values in the current column. All columns other than the current column become inputs and the current column becomes the target. The value of the current column in the records with NaN in the current column have to be predicted using the remaining records whose current column has no NaN values as training data
	5. Step 5: Calculate the difference between the data before the current iteration and data produced by the current iteration. Continue performing step 2 until the difference for every NaN value (according to the initial data) becomes zero or closes in on zero