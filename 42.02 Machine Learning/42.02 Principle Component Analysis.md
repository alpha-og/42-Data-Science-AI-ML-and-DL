# Principle Component Analysis
---
- aka: PCA
- PCA is used in [[42.02 Unsupervised learning|unsupervised ML]] 
- It is a very reliable, tried and tested technique which has existed since the 1950s
- PCA facilitates dimensionality reduction by transforming higher dimensional data to an optimal lower dimension such that the essence of the data is preserved
- The number of principle components (PC) will always be less than or equal to the number of features in the dataset
- **Advantages:**
	1. Facilitates quicker execution of algorithms
	2. Facilitates visualisation of data in 2d or 3d

> [!tip] Identifying the Optimal Number of PCs
> The cumulative variance of all the selected PCs must be greater than or equal to $90\%$. The optimal number of PCs must satisfy this criteria

- **Limitation:** The shape of data in some scenarios maybe such that dimensionality reduction through PCA does not solve the problem and may even worsen the issue. Example: ![[Screenshot 2023-08-14 at 8.12.00 PM.png]]
## Geometric Intuition
- Reference video: https://youtu.be/iRbsBi5W0-c
- For example, consider the scatter plot of the two features in 2d space whose initial variance are approximately equal and thus it is difficult to determine which feature must be selected if we were to reduce dimensionality through feature selection
- By performing PCA, we rotate the coordinate axis leading to a change in the variances (the variance will no longer be similar for the two features)
- The newly formed axes are termed as PC-1 and PC-2
- In the event that variance of PC-1 was greater than that of PC-2, then we would transform the feature corresponding to PC-1 with respect to PC-1
## Problem Formulation
- Reference video: https://youtu.be/tXXnxjj2wM4
- The datapoints in an $n$ dimensional space can be considered to be vectors
- In PCA we attempt to find a vector $\overset{\rightarrow}\mu$ such that the projection of all vectors corresponding to the datapoints in the dataset on $\overset{\rightarrow}\mu$ leads to a set of scalars whose variance is maximum
- We take the projection of mean of the original dataset on $\overset{\rightarrow}\mu$ and consider that to be the mean of the new set of scalars
- Mathematically, the objective function for PCA becomes: $$\sigma^{2}= \frac{\overset{n}{\underset{i=1}\Sigma}{(\overset{\rightarrow}\mu\cdot\overset{\rightarrow}x_i-\overset{\rightarrow}\mu\cdot\overset{\rightarrow}{\overline{x}})^2}}{N}$$
- $\overset{\rightarrow}\mu$ is determined by eigen decomposition. Read on about [eigen decomposition](https://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#:~:text=covariance%20matrix%20captures%20the%20spread%20of%20N%2Ddimensional%20data.&text=Figure%203.,is%20captured%20by%20the%20variance) and its association with PCA
## Obtaining Principle Components
- **Step 1:** Center the data about their mean ([[64.01 Standardisation|Standardisation]])
- **Step 2:** Find covariance matrix (use `np.cov()` method) 
- **Step 3:** Find eigen value and eigen vectors for covariance matrix
- Each eigen vector is considered as a principle component (PC). The PCs are numbered from 1 to n in  decreasing order of their value. We may consider all or some specific PCs based on how many dimensions we wish to reduce the problem to
## Transforming Datapoints
- Reference video clip: https://youtu.be/tXXnxjj2wM4?t=2630
- Once the PCs are obtained we must transform the datapoints in accordance with the chosen PCs
- For achieving this we take the dot product of the eigen vectors with each datapoint. In terms of matrices, we transpose the matrix of eigen vectors and then multiply it with the matrix of datapoints
