# Outliers
---
- Outliers are those values which are extremely different from the rest of the dataset and their removal leads to greater [[42.02 Generalisation|generalisation]] of the model ^01b500
- Scenarios where outliers are problematic
	- When the datapoint takes up values beyond what is considered to be physically feasible or reasonable
- Scenarios when outliers are useful:
	- When the problem requires the model to be able to identify outliers (example in the case of bank transactions, where transactions by unauthorised personnel are outliers ) — anomaly detection
	- When outliers can provide useful insight about the data
- Outliers can impact performance of the following ML algorithms (these are some examples which have a common theme, i.e, all of them involve calculation of weights)
	- Linear regression
	- Logistic regression
	- Ada Boost
	- Deep learning
- Outliers have very minimal impact on tree based algorithms
- The following are some techniques for treating outliers:
	1. Trimming
	2. Capping
	3. Consider outliers as missing values and proceed with [[42.02 Handling Missing Values|handling missing values]]
	4. Discretisation
- The  following are some techniques for detecting outliers:
	1. Using Z-score
	2. Using minimum and maximum based on IQR
	3. Using percentiles

## Outlier Detection
- Removal of outliers is a rather simple process, however their detection can be difficult as often times we are unaware of which data points may act as outliers unless we analyse the data
### Z-Score Method
- **Assumption:** data is normally distributed
- In order to detect outliers, the [[64.01 Standardisation#^f28bfd|z-score]] of each data point is calculated and any data point whose z-score lies beyond 3 standard deviations from the mean is considered to be an outlier
### IQR Proximity Method
- **Assumption:** data is a skewed distribution
- In order to detect outliers, the [[64.01 Describing Distributions of Numeric Variables#^891ed5|IQR]] for the column is calculated and the maximum and minimum values for that column are calculated. All values that lie above or below the calculated maximum or minimum, respectively are considered to be outliers
- We can visualise the outliers in a dataset using [[64.01 Variables#Box Plots|box plots]]
### Percentile Method
- This method involves choosing arbitrary upper and lower thresholds for the dataset in terms of percentile and then trimming or capping the datapoints that lie outside the range of values between the chosen percentile thresholds
## Outlier Treatment
### Trimming
- Trimming involves removal of all the outliers from the dataset
- However, trimming is not preferred since it removes entire records regardless of the utility of the features of that record
### Capping
- Capping involves introducing a maximum and minimum to the values of each column such that anything above the maximum or below the minimum get limited to the maximum or minimum, respectively
- This approach is much better than trimming since the records still are part of the dataset
- For Z-score method:
	- For direct application on raw data — Maximum: $\mu +3\sigma$  | Minimum: $\mu -3\sigma$
	- For [[64.01 Standardisation|standardised]] data — Maximum: $3$ | Minimum: $-3$
- For IQR Proximity method:
	- Maximum: $Q3+1.5IQR$
	- Minimum: $Q1-1.5IQR$
- For Percentile method:
	- Maximum: value of column at $99\%ile$
	- Minimum: value of column at $1\%ile$
	- aka: *Winsorization* (capping is known by this term in the context percentile method)
	- When capping values with percentiles, we replace those value equal to or above the maximum with a value that lies just above the maximum and those values equal to or below the minimum are replaced with a value that lies just below the minimum