# Generalisation
---
- **Generalisation** refers to a model's ability to adapt properly to new, previously unseen data, drawn from the same distribution as the one used to create the model.
- Also check out (not necessary): [Generalisation Theory](https://en.wikipedia.org/wiki/Generalization_(learning)#:~:text=Generalization%20allows%20humans%20and%20animals,in%20new%20situations%20and%20environments.)
- Instead of using the theoretical aspect of Generalisation Theory, we use the empirical strategy. This involves training a model on a training dataset and then providing the model an unseen example input to evaluate its prediction
- A model capable of predicting just as well in the test set (unseen examples) as it does in the training set is said to be able to "generalise" well against unseen data
- Three basic assumptions for all of the above:
	1.  We draw examples **independently and identically (i.i.d.)** at random from the distribution
	2.  The distribution is **stationary**: It doesn't change over time
	3.  We always pull from the **same distribution**: Including training, validation, and test sets
- From the aforementioned assumptions one or more may be violated in real life scenarios as in the case with customer behaviour or any other changing variable. In such cases one must pay careful attention to the metrics
- [[42.02 Introduction#^1e2789|Overfitting]] is often caused due to a model being more complex than it needs to be

> [!tip] Why is generalisation important?
> This [article](https://developers.google.com/machine-learning/crash-course/generalization/peril-of-overfitting) goes over the significance of generalisation and explains the concept of [[42.02 Introduction#^1e2789|overfitting]]

> [!quote] Ockham's Razor in the context of ML
> The less complex an ML model, the more likely that a good empirical result is not just due to the peculiarities of the sample.

^180d5e

- **Generalisation bounds** are a statistical description of a model's ability to generalise to new data based on factors such as:
	- the complexity of the model
	- the model's performance on training data
## Splitting the Dataset
- In order to assess the capability of a model to generalise well against unseen examples, one of the methods that can be used is to divide the dataset into a training set and test set
	- **Training set** — a subset to train a model.
	- **Test set** — a subset to test the trained model.
- Splitting the dataset in such a way can prove useful, provided:
	- the initial dataset is large enough
	- the model is not trained on the test set repeatedly
- When splitting the dataset it is also important to randomise the sequence of each entry in the dataset so as to prevent data of a similar kind all being present in either the training or test set
- The size of the training and test sets can influence the capabilities of the model:
	- The larger the training set the better the model is able to learn
	- The larger the test set the better our confidence in the model's ability to make correct predictions
- Generally (in case of large datasets) the dataset is split such that the training set comprises 85-90% of the dataset while the test set is the remaining 10-15%
- If the available dataset is small, then more sophisticated methods such as cross-validation may have to be carried out
- Avoid (DO NOT) training the model against the test set as it can give false perceptions of the model's accuracy. Even when we split the test data properly some duplicate entries can cause the model to be trained on the test set as well; hence it is also advisable to cleanup the data and remove duplicate entries
> [!important] Make sure that the test set meets the following two conditions:
> 
> -   Is large enough to yield statistically meaningful results.
> -   Is representative of the data set as a whole. In other words, don't pick a test set with different characteristics than the training set.

- Another problem that can come up when using just two splits on the dataset is that when we tweak different hyperparameters to reduce the loss against the test set on a model which was trained on the training set, we may implicitly train it to adjust to the peculiarities of the test set
- To overcome the aforementioned hurdle, we can introduce a third partition to our dataset called a **validation set**. The validation set can be used to evaluate our model after it is trained on the training set while the test set is left untouched. We can tweak the hyperparameters so as to reduce loss against the validation set and finally when we achieve similar or equal losses for the model against both training and validation set, we may then pass the test data to it, to determine the confidence of the model
- The process of "tweaking", mentioned previously, involves anything, ranging from changing the learning rate, to adding or removing features, to designing a completely new model from scratch
- The workflow we adopt ![[Screenshot 2023-05-28 at 8.23.01 PM.png]]
> [!tip]
> Test sets and validation sets "wear out" with repeated use. That is, the more you use the same data to make decisions about hyperparameter settings or other model improvements, the less confidence you'll have that these results actually generalize to new, unseen data.
> 
> If possible, it's a good idea to collect more data to "refresh" the test set and validation set. Starting anew is a great reset.
