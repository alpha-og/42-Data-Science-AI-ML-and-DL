# Learning Rate
---
- Another important parameter used is the learning rate which effectively determines how small of a step the model takes in correcting its hyperparameter values in accordance with corresponding negative of gradients
- A small learning rate would take significant amount of time to compute and achieve minimum loss while a very large learning rate would cause the hyperparameter value changes to overshoot the local minima of the loss function
- Thus it is necessary to set an appropriate learning rate which is neither too small nor too large
> [!note]
> - The ideal learning rate in one-dimension is $$\frac{1}{f''(x)}$$ (the reciprocal of the second derivative of $f(x)$ at $x$).
> - The ideal learning rate for 2 or more dimensions is the inverse of the [Hessian](https://wikipedia.org/wiki/Hessian_matrix) (matrix of second partial derivatives)
> - The story for general convex functions is more complex.