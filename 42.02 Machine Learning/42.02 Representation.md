# Representation
---
- The data available to us may not necessarily be comprehendible to the computer
- In such scenarios appropriate representation of the input data is necessary so that the ML model is able to make sense out of the data
- More importantly, it is necessary to choose the right features to train a model
- The process of extracting useful features from raw data is called **feature engineering**
- When the feature values are numeric, they may be copied to the feature set directly from the raw data with little or no processing. However, if the feature values are non-numeric (example: string) then the feature value must be encoded ([[42.01 Encoding Inputs#One_Hot Encoding|one-hot encoding]] is an example of a possible method of encoding)
- Numeric feature values too can be one-hot encoded if the requirement is such that it is better to avoid directly passing the feature values into the model
- Useful features are those features:
	- which take up non-zero values more than a few times (5 or more) in a dataset
	- whose values take up reasonable values that make sense for the situation at hand
	- whose definition does not change over time
	- which do not take up magic values (use a boolean to identify and track such values)
- The feature data maybe binned (grouped) and then encoded to give useful features

> [!tip] Know your data
> - Do not treat ML as a magic machine that gives reliable and correct outputs given any input
> - It is important to visualise and inspect the data to identify any biases and quirks so that we can build a model that accounts for the same
> - Cleaning up the data (removing redundant entries, analysing how the data is sorted and shuffling them, etc...)
> - Monitor the data (in the case of a dataset which is continuously changing); this helps keep a check on the quality of the data as what was good yesterday needn't be good today

- The features in most cases are converted into a vector containing real values so that they can be multiplied with weights of the neural network. This is the essence of feature engineering
- Feature engineering can take up a significant amount of time 
> [!tip] Sparse Representation 
> - When the vocabulary size is very large, creating and storing vectors of large size becomes inefficient and computationally taxing
> - In such scenarios sparse representation can be used where the representations stores only the position (index) of non-zero values in the vector
> - Such a representation still allows the model to learn independent weights for each element in the vocabulary list
> - It is important to consider that we may not pass the sparse representation directly into our model and must instead be converted into a one-hot encoding prior to training  ^f1c9e8

## Scaling
- Reference: https://developers.google.com/machine-learning/crash-course/representation/cleaning-data
- **Scaling**Â means converting floating-point feature values from their natural range (for example, 100 to 900) into a standard range (for example, 0 to 1 or -1 to +1).
- Scaling is useful when a feature set consists of multiple features and isn't as useful in the case of a feature set containing a single feature
- Advantages of Scaling:
	1. Helps gradient descent converge quickly
	2. Prevents occurrence of undefined/ NaN values due to the value exceeding the size specified by the datatype
	3. Helps the model learn appropriate weights for each feature

> [!note] 
> It is not necessary to scale every feature in the same manner

- Ways of scaling:
	1. Taking the logarithm
	2. Using the z score $Z\ score = \frac{x_i-\overline{x}}{\sigma}$
	3. Using any function whose range lies within a certain standard range
- Scaling can be used to handle outliers (extreme values). 

> [!tip] Clipping
> - Another approach to handle outliers is to simply clip the values that exceed a certain threshold (varies across situations)
> - By doing so we do not ignore the values that lie beyond the threshold, we simply make those values equal to the maximum threshold value

## Scrubbing
- Scrubbing involves cleaning out the raw data to remove any undesirable values that can cause havoc with training the ML model
- The necessity of scrubbing arises from:
	- Omitted values (missing feature values)
	- Redundant examples
	- Bad labels (mis-labeled data)
	- Bad values (values being beyond reasonable limits)
- *Omitted values* and *redundant examples* can be identified and removed fairly easily. However, examples with bad labels and values are far more difficult to identify
- In order to detect bad labels and values we may take the assistance of statistical data such as minimum, maximum, mean, etc... and consider whether these values and the values of the dataset make sense in the real world