# Curse of Dimensionality
---
- The features/ columns in dataset are often referred to as *dimensions*
- There is a point of diminishing returns when we keep adding more and more features to the dataset
- According to the curse of dimensionality, there is an optimal number of features required to be present in a dataset for the model to make accurate and correct predictions, beyond which the benefits decline and even may cause harm to the model
- **Example:**
	- Consider the MNIST dataset, having photographs of handwritten numbers. Each pixel in a photo is a feature
	- If the model were to analyse the entire image, the whitespace surrounding the numbers would simply add unnecessary burden on the model and reduce its efficiency and may even create wrong implications
	- If the model were to analyse only a small fraction of the number, then predictions would be inaccurate
	- Thus there exists an optimal number of features, in this case this is the smallest possible boundary surrounding the number
- Increase in dimensionality leads to sparsity of the data, i.e, the data becomes more spread apart as the number of dimensions increase
- Since most ML models rely on statistical techniques, prediction of target values using the data becomes tougher with increasing dimensionality beyond the optimal number of features
## Dimensionality Reduction
- **Dimensionality Reduction** is the process of transforming high dimensional data to a lower dimension
- In order to overcome the curse of dimensionality we may adopt the process of  dimensionality reduction which involves (can use either or both):
	1. [[42.02 Feature Engineering#Feature Selection|Feature Selection]]
	2. [[42.02 Feature Engineering#Feature Extraction|Feature Extraction]]
- Variance is an important measure to consider during dimensionality reduction. Example:
	- Consider the problem of reducing a 2D dataset to 1D
	- We have to options, project the data onto the x-axis or the y-axis
	- We see that the distance between 2 points in the dataset is $x$ and the variance for the feature plotted against x-axis is $\sim x$, however on the y-axis the variance is $<<<<x$ 
	- Thus reducing dimensionality based on the feature plotted against the x-axis would be more desirable as the distance between the two points is preserved in this case
	- Watch this clip to understand in detail: https://youtu.be/iRbsBi5W0-c?t=1828