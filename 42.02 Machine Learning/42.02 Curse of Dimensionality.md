# Curse of Dimensionality
---
- The features/ columns in dataset are often referred to as *dimensions*
- There is a point of diminishing returns when we keep adding more and more features to the dataset
- According to the curse of dimensionality, there is an optimal number of features required to be present in a dataset for the model to make accurate and correct predictions, beyond which the benefits decline and even may cause harm to the model
- **Example:**
	- Consider the MNIST dataset, having photographs of handwritten numbers. Each pixel in a photo is a feature
	- If the model were to analyse the entire image, the whitespace surrounding the numbers would simply add unnecessary burden on the model and reduce its efficiency and may even create wrong implications
	- If the model were to analyse only a small fraction of the number, then predictions would be inaccurate
	- Thus there exists an optimal number of features, in this case this is the smallest possible boundary surrounding the number
- Increase in dimensionality leads to sparsity of the data, i.e, the data becomes more spread apart as the number of dimensions increase
- Since most ML models rely on statistical techniques, prediction of target values using the data becomes tougher with increasing dimensionality beyond the optimal number of features
- In order to overcome the curse of dimensionality we may adopt the process of **Dimensionality Reduction** which involves (can use either or both):
	1. Feature Selection
	2. Feature Extraction