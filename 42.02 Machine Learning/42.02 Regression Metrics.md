# Regression Metrics
---
- **Regression Metrics** are measures that we way use to gauge the effectiveness, accuracy and other parameters of a model concerning its utility in real life
- Reference video: https://youtu.be/Ti7c-Hz7GSM
- The following are some regression metrics:
	1. Mean Absolute Error
	2. Mean Squared Error
	3. Root Mean Squared Error
	4. R2 Score
	5. Adjusted R2 Score
## Mean Absolute Error
- aka: MAE
- MAE involves finding the *absolute* distance of each datapoint from the regression curve and calculating their mean
- $$MAE=\frac{\overset{N}{\underset{i=1}\Sigma}|y_i-\hat{y}_i|}{N}$$ where $y_i$ is the actual value of $i^{th}$ target variable and $\hat{y}_i$ is the predicted value of $i^{th}$ target variable
- **Advantages:**
	1. The unit of the error is the same as that of the target variable
	2. Robust against outliers
- **Disadvantages** The modulus function used in obtaining the absolute value is non-differentiable at the origin (many algorithms rely on differentiation and other principles of calculus)

## Mean Squared Error
- aka: MSE
- MSE involves finding the *square* of distance of each datapoint from the regression curve and calculating their mean
- $$MSE=\frac{\overset{N}{\underset{i=1}\Sigma}(y_i-\hat{y}_i)^2}{N}$$ where $y_i$ is the actual value of $i^{th}$ target variable and $\hat{y}_i$ is the predicted value of $i^{th}$ target variable
- Geometrically, MSE calculates the mean of areas of squares representing the distance of each point from the regression curve
- **Advantages:** Differentiable
- **Disadvantages:** 
	1. The units are not same as that of target variable
	2. Sensitive to outliers as it amplifies the impact of outliers

## Root Mean Squared Error
- aka: RMSE
- RMSE involves finding the *square* of distance of each datapoint from the regression curve, calculating their mean and then finding the square root
- $$RMSE=\sqrt{\frac{\overset{N}{\underset{i=1}\Sigma}(y_i-\hat{y}_i)^2}{N}}$$ where $y_i$ is the actual value of $i^{th}$ target variable and $\hat{y}_i$ is the predicted value of $i^{th}$ target variable
- **Advantages:** 
	1. Differentiable
	2. The units are same as that of the target variable
- **Disadvantages:** Sensitive to outliers

## R2 Score
- R2 Score is an absolute metric which in independent of the context, i.e, its output can be considered to be definitive of the model's efficacy irrespective of the circumstances
- The fundamental concept of R2 score is the comparison of the worst case scenario of the model to the current iteration of the model
- In the worst case scenario, the model would have no input features and would simply have to make predictions based on the mean of the target variable. 
- R2 score compares the predictions of the machine learning model with the worst case prediction (mean in most cases) to determine the accuracy of the model
- The sum of squared errors against the *regression curve* is termed as $SS_R$
- The sum of squared error against the *mean line* is termed as $SS)M$
- $$\begin{align}
\text{R2 Score} &= 1-\frac{SS_R}{SS_M}\\
&=1-\frac{\overset{N}{\underset{i=1}\Sigma}{(y_i-\hat{y}_i)^2}}{\overset{N}{\underset{i=1}\Sigma}{(y_i-\overline{y})^{2}}}
\end{align}$$ where $\overline{y}$ is the mean of target variable (worst case prediction) and $\hat{y}_{i}$ is the predicted value of the regression model for the target variable
### Interpretation of the R2 Score
- R2 Score will be zero if the MSE for the regression curve and the MSE for the mean value prediction are equal, i.e, $SS_{R}=SS_{M}$ and $\frac{SS_R}{SS_M}=1$
- R2 Score will be unity if MSE for the regression curve is zero or the MSE for the mean value prediction is infinite (not possible, I think)
- R2 Score will be negative if MSE for the regression curve exceeds the MSE for the mean value prediction, i.e, $SS_R>SS_M$ and $\frac{SS_R}{SS_M}>1$
- As the R2 score tends to unity, the model becomes more accurate
- If the R2 score for given dataset and ML model is $0.8$ (for example) then the interpretation is that, the chosen features in the training data are able to explain upto $80\%$ of variance in the target variable's values

## Adjusted R2 Score
- Adjusted R2 score solves a limitation of the R2 score as R2 score is susceptible to inclusion of irrelevant features, i.e the R2 score may increase or remain the same instead of decreasing when irrelevant features are included (such as temperature of the day is included in a dataset of CGPA and LPA of students which causes the R2 score to increase, although temperature doesn't have nay direct causal relationship with CGPA or LPA when predicting LPA)
- $$\begin{align}
\text{Adjusted R2 Score} &= 1-\frac{(1-\text{R2})(N-1)}{(N-1-k)}
\end{align}$$ where $N$ is the total number of entries in the dataset and $k$ is the total number of independent features/ columns
- Explanation for how adjusted R2 score solves the problem with R2 score: https://youtu.be/Ti7c-Hz7GSM?t=1796
