# Feature Engineering
---
- Feature engineering involves the following aspects:
	1. Feature Transformation
	2. Feature Construction
	3. Feature Selection
	4. Feature Extraction
## Feature Transformation
- Feature transformation involves the transformation or change of available data to a form that would prove to be much more apt for the ML algorithm
- This includes:
	- missing value imputation
	- [[42.02 Feature Encoding|Feature Encoding]]
	- outlier detection
	- [[42.02 Feature Scaling|Feature Scaling]]
### Missing Value Imputation
- Missing values in datasets can cause undesirable outcomes with the ML models and in order to avoid such scenarios we must fill the missing values with values such as the mean, median, etc... 
### Outlier Detection
- Outliers are those values which are extremely different from the rest of the dataset and their removal leads to greater [[42.02 Generalisation|generalisation]] of the model ^01b500
- Removal of outliers is a rather simple process, however their detection can be difficult as often times we are unaware of which data points may act as outliers unless we analyse the data
## Feature Construction
- Feature construction involves creation of new features using already available features
- An example of this is [[42.02 Feature Crosses|feature cross]]
- The process of feature construction is an intuition driven process which is akin to an art since there is no definitive method to follow
## Feature Selection
- Feature selection is the process of selecting or plucking out useful features from a large array of features
- Feature selection can greatly speed up the time taken to train models
- However, it is important to note that picking the right features is necessary to produce desirable outcomes
## Feature Extraction
- Feature extraction helps in reducing the number of features by introducing a new feature altogether that covers the purpose of those features it replaces
- Some algorithms used in this process are: PCA, LDA, TSME