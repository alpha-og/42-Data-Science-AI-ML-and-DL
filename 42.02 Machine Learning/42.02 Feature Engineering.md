# Feature Engineering
---
- Feature engineering involves the following aspects:
	1. Feature Transformation
	2. Feature Construction
	3. Feature Selection
	4. Feature Extraction
## Feature Transformation
- Feature transformation involves the transformation or change of available data to a form that would prove to be much more apt for the ML algorithm
- This includes:
	- missing value imputation
	- handling categorical features
	- outlier detection
	- [[42.02 Feature Scaling|Feature scaling]]
### Missing Value Imputation
- Missing values in datasets can cause undesirable outcomes with the ML models and in order to avoid such scenarios we must fill the missing values with values such as the mean, median, etc... 
### Handling Categorical Features
- In cases where the data refers to categories such as different animals or age groups, it becomes necessary to convert the categorical data into an encoded form which is numerical
- The converse may also be required depending on the situation
- [[42.02 Introduction#Data|Categorical data]] is usually in the form of strings which cannot be directly used in an ML algorithm
- Two of the most common encoding techniques:
	1. Ordinal Encoding (used for handling [[42.02 EDA#^dbf8ae|Ordinal Categorical Data]])
	2. One Hot Encoding (used for handling [[42.02 EDA#^0012b7|Nominal Categorical Data]])
#### Ordinal Encoding
- In order to encode input features that are ordinal categorical data, ordinal encoding can be used
- However, if the target is categorical, then instead of ordinal encoding, label encoding is done
- In ordinal encoding, a number is assigned to each unique category such that the number is indicative of its precedence (i.e, categories with higher significance or importance in the context of the problem, are given higher values)
#### One Hot Encoding (OHE)
- OHE is used for encoding nominal categorical data, since ordinal encoding can cause the model to incorrectly assume priorities for the data even though there is no definite order
- OHE creates a vector that has as many dimensions as the number of unique categories in the input feature
- *Dummy Variable Trap:*
	- When using OHE, we must make sure that the encoded input features are independent of each other (as the inputs are *independent variables*) to avoid multi-collinearity
	- In order to overcome this we can simply drop any one of the columns (i.e, total number of columns in the final encoded inputs will be `n-1` where `n` is the total number of nominal categorical data columns prior to encoding). This is effective due to the dependence being removed and the data still represents the original data since we can continue to interpret the removed column's data based on the new encodings
	- Example: ![[Screenshot 2023-07-17 at 6.50.22 PM.png]]
- Tackling datasets whose nominal categorical data have a large number of distinct categories:
	- If we were to proceed with traditional OHE, the encoded inputs would have a large dimensionality which can increase storage requirements and decrease computation speed
	- In order minimise the resource requirements, after performing OHE of required inputs, we may discard the categories which occur very rarely and group them into a separate class such as `others`
	- This helps in reducing dimensionality
### Outlier Detection
- Outliers are those values which are extremely different from the rest of the dataset and their removal leads to greater [[42.02 Generalisation|generalisation]] of the model ^01b500
- Removal of outliers is a rather simple process, however their detection can be difficult as often times we are unaware of which data points may act as outliers unless we analyse the data
## Feature Construction
- Feature construction involves creation of new features using already available features
- An example of this is [[42.02 Feature Crosses|feature cross]]
- The process of feature construction is an intuition driven process which is akin to an art since there is no definitive method to follow
## Feature Selection
- Feature selection is the process of selecting or plucking out useful features from a large array of features
- Feature selection can greatly speed up the time taken to train models
- However, it is important to note that picking the right features is necessary to produce desirable outcomes
## Feature Extraction
- Feature extraction helps in reducing the number of features by introducing a new feature altogether that covers the purpose of those features it replaces
- Some algorithms used in this process are: PCA, LDA, TSME