# Classification
---
- **Classification** is the process of predicting an output from a finite set of possible outputs
- A **classification** model predicts discrete values (contextually, the predictions are categories). For example, classification models make predictions that answer questions like the following:
	- Is a given email message spam or not spam?
	- Is this an image of a dog, a cat, or a hamster?
## Evaluation
- Metrics for evaluating classification models:
	1. Accuracy (total correct predictions/ total labels)
	2. Precision (the ratio of correct positive predictions to total predicted positives)
	3. Recall (the ratio of correct positive predictions to all actual positives)
- **Accuracy** is the fraction of predictions made by the model that were correct $$\text{Accuracy} = \frac{\text{Number of correct predictions}}{\text{Total number of predictions}}=\frac{TP + TN}{TP + TN + FP + FN}$$
- **Precision** is the fraction of the predicted positives that was actually true  $$\text{Precision} =\frac{TP}{TP+FP}$$
- **Recall** is the fraction of the actual positives that was correctly predicted to be positive $$\text{Recall} = \frac{TP}{TP+FN}$$

> [!warning] Accuracy is not a one size fits all solution
> 1. Different kinds of mistakes have different kinds of consequences (read [this](https://developers.google.com/machine-learning/crash-course/classification/accuracy) for an example based reasoning for the same )
> 2. Class imbalance can cause the model to be biased towards a certain prediction (like in the case of click through rates where the probability of the user clicking an ad is extremely low to the point where the model might predict that a person will not click the model with 99% accuracy)
> 3. For class imbalance problems, it is beneficial to classify the errors into 4 different categories ![[Screenshot 2023-06-11 at 8.09.21 AM.png]]

- Precision and Recall values remain proportional with change in number of TPs but are inversely proportional to number of FPs and number of FNs respectively
- The number of FPs and FNs are inversely related; i.e, an increase in number of FPs causes a decrease in FNs and vice-versa
- Owing to the inverse relation of Precision with number of FPs and Recall with number of FNs, Precision and Recall are also inversely related; i.e, an increase in precision leads to a decrease in recall and vice-versa
- If a model produces no false negatives then it has a recall of $1$
## Classification Threshold
- The classification threshold is the minimum predicted probability above which a classification can be made
- It will be different in different scenarios and one must learn to fine tune it to the problem in hand
- One way of identify the correct threshold is to try every possible threshold and track the true positive and false positive rates. This is represented by the ROC (Receiver Operating Characteristics) curve 
- The **ROC Curve** is a graph showing the performance of a classification model at all classification thresholds. It plots the TPR against FPR
- **True Positive Rate** (TPR) is the same as recall $$TPR = \frac{TP}{TP+FN}$$
- **False Positive Rate** (FPR) is the fraction of actual negatives that was incorrectly predicted to be positive $$FPR = \frac{FP}{FP + TN}$$
- The area under the ROC curve gives us the probability of error
- Example of a typical ROC ![](https://developers.google.com/static/machine-learning/crash-course/images/ROCCurve.svg)
- In order to plot the ROC curve, the direct approach would be to plot the individual points, however such an approach is highly in-efficient. Instead, a sorting based algorithm called Area Under the ROC Curve (AUC) is used
- AUC is the the area enclosed between the ROC curve and the positive x-axis (definite integral from $(0,0)$ to $(1,1$) $$AUC\ \epsilon\ [0,1]$$
- AUC provides an aggregate measure of performance across all possible classification thresholds. It can be interpreted as the probability with which the model reports an actual positive over an actual negative
- A model whose predictions are 100% wrong has $AUC=0$ 
- A model whose predictions are 100% correct has $AUC=1$

> [!tip] Benefits of AUC
> - AUC is **scale-invariant**. It measures how well predictions are ranked, rather than their absolute values.
> - AUC is **classification-threshold-invariant**. It measures the quality of the model's predictions irrespective of what classification threshold is chosen.

> [!warning] Caveats of AUC
> - **Scale invariance is not always desirable.** For example, sometimes we really do need well calibrated probability outputs, and AUC won’t tell us about that.
> - **Classification-threshold invariance is not always desirable.** In cases where there are wide disparities in the cost of false negatives vs. false positives, it may be critical to minimise one type of classification error. For example, when doing email spam detection, you likely want to prioritise minimising false positives (even if that results in a significant increase of false negatives). AUC isn't a useful metric for this type of optimization.

## Prediction Bias
- **Zero Bias/ Unbiased:** Ideally the model should be unbiased and the average of predictions would equal the average of observations
- Having zero bias does not necessarily indicate that the model is perfect, but it can be used to roughly estimate if the model is functioning rationally
- Not having zero bias in a rather complex model is unusual and can help direct us in the right direction since such a situation is indicative of an internal issue with the model
- In order to rectify possible problems in a model due to non zero bias we can slice the data and try to understand the implications of different slices of data on the model
- Prediction bias is a measure of how far apart the average of predictions is from the average of observations $$\text{Prediction Bias = Avg of Predictions - Avg of Observations}$$
> [!note]
> Prediction Bias is different from the bias in the context of an artificial neuron

- Possible root causes of prediction bias are:
	- Incomplete feature set
	- Noisy data set
	- Buggy pipeline
	- Biased training sample
	- Overly strong regularisation

> [!warning] Calibration Layers
> - One way to fix the prediction bias is through the use of calibration layers. Calibration layers are layers added to the neural network to decrease or increase mean by a certain amount so as to make the mean of predictions equal to the mean of observations
> - While this may seem convenient to do, it can turn out to be cumbersome and complicated as the model changes. Thus it is advisable to avoid the use of calibration layers to prevent situations where we have to change calibration layer frequently to correspond to the input data
> - Moreover the use of calibration layers only address the symptom instead of the root cause

