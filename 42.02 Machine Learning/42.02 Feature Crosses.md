# Feature Crosses
---
- Reference: https://developers.google.com/machine-learning/crash-course/feature-crosses/encoding-nonlinearity
- This involves creating synthetic features from pre-existing/ available feature data
- In general, this can be considered akin to taking the cross product of two or more features
- In the case of features which take up boolean values, the one hot encoding of their feature crosses can contain an awful lot of zeros
- This method allows encoding non-linearity into a linear model
- Linear models are important since they can scale very well to very large datasets
- If the initial model was represented by the following equation: $y=w_{1}x_{1}+w_{2}x_{2} +b$; then the new model comprising the feature cross $x_3=x_1x_2$ is represented as follows: $$y=w_{1}x_{1}+w_{2}x_{2}+w_{3}x_{3} +b$$
- A feature cross can be of many different forms such as:
	- a cross of one feature with itself
	- a cross of one feature with another
	- a cross of one feature with several other crosses
- [[42.02 Loss#^c026b0|SGD]] enables linear models to be trained efficiently and supplementing linear models with feature crosses has been used to efficiently train models on large datasets
- Feature crosses between [[42.01 Encoding Inputs#One_Hot Encoding|one-hot encodings]] can be thought of as a logical conjunction (consider cross product of two sets which outputs a new set whose size is the product of individual sizes of the sets)
- Have a look at this to have a programmatic understanding of feature cross and its implications: [Representation with Feature Crosses](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/representation_with_a_feature_cross.ipynb?utm_source=mlcc&utm_campaign=colab-external&utm_medium=referral&utm_content=representation_tf2-colab&hl=en)
> [!tip] Dont get carried away!
> Avoid over-use of feature crosses as it can cause the model to give lower losses in the training set as the model adapts to the peculiarities of the training set; and as a result it can yield higher losses in the test set when compared to a model that used an optimal number of feature crosses or none at all
